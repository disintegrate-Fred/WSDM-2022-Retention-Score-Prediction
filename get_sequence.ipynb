{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool as ProcessPool\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqget_data_from_id( uid ):\n",
    "    dfs = []\n",
    "    for end in data_dict[uid]:\n",
    "        launch_type , date = launch_dict[ uid ]\n",
    "        launch_type =launch_type[ date <= end ]\n",
    "        date = date[ date <= end ]\n",
    "        dfs.append( pd.DataFrame(pd.Series( { 'user_id' : uid , \n",
    "                                 'end_date' : end , \n",
    "                                 'launch_type' : launch_type , \n",
    "                                 'launch_date' : date} ) ).T )\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def seq_seq( df , need_2 = True ):\n",
    "    def check( row , e ):\n",
    "        if e in launch_dic[ row.user_id ]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def func( row ):\n",
    "        end_date = row.end_date\n",
    "        f = lambda x : 1 if x > 0 else 0\n",
    "        if not need_2 : \n",
    "            seq = [ f(e) for e in row.launch_seq]\n",
    "        else:\n",
    "            seq = [ e for e in row.launch_seq]\n",
    "        future = [ check( row , e ) for e in range( end_date + 1 , end_date + 8 )  ]\n",
    "        return pd.Series( { 'launch_seq' : seq , 'end_date' : end_date ,  \n",
    "                           'future' : future , 'label_list' : row.label_list } ).T\n",
    "    data = df.apply( lambda x : func(x) , axis=1 )\n",
    "    data = data.reset_index()\n",
    "    del data['index']\n",
    "    return pd.concat([ data , pd.DataFrame(df.user_id).reset_index() ] , axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "playback = pd.read_csv(\"data/user_playback_data.csv\", dtype={\"item_id\": str})\n",
    "playback = playback.sort_values( 'date' )\n",
    "video_data = pd.read_csv(\"data/video_related_data.csv\", dtype=str)\n",
    "playback = playback.merge(video_data[video_data.item_id.notna()], how=\"left\", on=\"item_id\")\n",
    "playback.tail()\n",
    "playback_grp = playback.groupby([\"user_id\"]).agg(\n",
    "    playtime_list=(\"playtime\", list ),\n",
    "    item_seq = (\"item_id\", list ),\n",
    "    date_list=(\"date\", list ),\n",
    "    duration_list=(\"duration\", list),\n",
    "    father_id_list=(\"father_id\", list),\n",
    "    tag_list=(\"tag_list\", list),\n",
    "    cast_list=(\"cast\", list)\n",
    ").reset_index()\n",
    "playback_grp[ 'playtime_list' ] = playback_grp.playtime_list.apply( lambda x : np.array(x) )\n",
    "playback_grp[ 'item_seq' ] = playback_grp.item_seq.apply( lambda x : np.array(x) )\n",
    "playback_grp[ 'date_list' ] = playback_grp.date_list.apply( lambda x : np.array(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle( playback_grp , 'playback_grp.pkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch = pd.read_csv(\"data/app_launch_logs.csv\")\n",
    "launch_gp = launch.groupby(\"user_id\")\n",
    "launch_grp = pd.concat( [launch_gp['date'].apply( np.array ) , launch_gp['launch_type'].apply( np.array ) ]  , axis = 1 ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle( launch_grp , 'launch_grp.pkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/online_testb.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600000it [00:20, 29884.57it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 33244.15it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 26132.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 17476.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 53869.05it/s]\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.64s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 23399.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 26973.02it/s]\n"
     ]
    }
   ],
   "source": [
    "where = 'online'\n",
    "for dataset in ['trainb', 'testb']:\n",
    "    print( f'data/{where}_{dataset}.csv' )\n",
    "    data = pd.read_csv( f'data/{where}_{dataset}.csv' )\n",
    "\n",
    "    launch_grp = load_pickle( 'launch_grp.pkl' )\n",
    "    data_dict = data.groupby( 'user_id' )['date'].agg(list).to_dict()\n",
    "\n",
    "    launch_dict = {}\n",
    "    for i , r in tqdm( launch_grp.iterrows() ):\n",
    "        launch_dict[r.user_id] = [ r.launch_type , r.date ]\n",
    "\n",
    "\n",
    "\n",
    "    pool = ProcessPool(10)\n",
    "    dfs = pool.map(seqget_data_from_id, list( data_dict.keys() ) )\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    tmpdata = pd.concat(dfs)\n",
    "\n",
    "    launch_grp = load_pickle( 'launch_grp.pkl' )\n",
    "    launch_grp.columns = [ 'user_id' ,  'launch_date'  ,  'launch_type' ]\n",
    "    launch_grp = launch_grp[ launch_grp.user_id.isin( tmpdata.user_id.unique() ) ]\n",
    "    launch_dic = dict( zip(launch_grp.user_id , launch_grp.launch_date ) )\n",
    "\n",
    "    dfs = df_split( tmpdata , 6000 )\n",
    "    pool = ProcessPool(8)\n",
    "    dfs = pool.map(fill_launch_seq, dfs )\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    tmpdata = pd.concat(dfs)\n",
    "\n",
    "    dfs = df_split( tmpdata , 10000 )\n",
    "    pool = ProcessPool(8)\n",
    "    dfs = pool.map(get_label_list, dfs )\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    tmpdata = pd.concat(dfs)\n",
    "\n",
    "    dfs = df_split( tmpdata , 10000 )\n",
    "    pool = ProcessPool(8)\n",
    "    dfs = pool.map(seq_seq, dfs )\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    seq = pd.concat(dfs)\n",
    "\n",
    "    tmpdata = seq.merge( playback_grp , on = 'user_id' , how = 'left'  )\n",
    "\n",
    "\n",
    "\n",
    "    ls = df_split( tmpdata,100000 )\n",
    "    res = []\n",
    "    for df in tqdm( ls ):\n",
    "        dfs = df_split( df,1000 )\n",
    "        pool = ProcessPool(10)\n",
    "        dfs = pool.map(seqmodifylist, dfs )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        res = res + list(dfs)\n",
    "        del pool\n",
    "        gc.collect()\n",
    "    tmpdata = pd.concat(ls)\n",
    "    dfs = df_split( tmpdata , 10000 )\n",
    "    pool = ProcessPool(8)\n",
    "    dfs = pool.map(seqget_playtime, dfs )\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    tmpdata = pd.concat(dfs)\n",
    "\n",
    "\n",
    "\n",
    "    tmpdata.playtime_seq = tmpdata.playtime_seq.apply( \n",
    "        lambda x : np.array( [0]*64 ) if isinstance(x,float) else np.array(x) )\n",
    "    dfs = df_split( tmpdata , 10000 )\n",
    "    pool = ProcessPool(8)\n",
    "    dfs = pool.map(new_seq, dfs )\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    tmpdata = pd.concat(dfs)\n",
    "\n",
    "    cols = [ 'user_id' , 'end_date' , 'future' , 'list' ]\n",
    "\n",
    "    tmpdata[cols].to_csv(f'data/{where}{dataset}seqex3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
